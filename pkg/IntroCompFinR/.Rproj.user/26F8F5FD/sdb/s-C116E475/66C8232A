{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Hypothesis testing in the CER Model\"\nauthor: \"Eric Zivot\"\ndate: \"July 30, 2015\"\noutput: slidy_presentation\n---\n\n## Set options and load packages\n\n```{r}\noptions(digits=3, width=70)\n# install IntroCompFinR package from R-forge\n# use install.packages(\"IntroCompFinR\", repos=\"http://R-Forge.R-project.org\")\nlibrary(IntroCompFinR)\nlibrary(mvtnorm)\nlibrary(PerformanceAnalytics)\nlibrary(tseries)\nlibrary(zoo)\nSys.setenv(TZ=\"UTC\")\n```\n\n## Get data from package IntroCompfinR\n\n```{r}\n# get data from IntroCompFin package\ndata(msftDailyPrices, sp500DailyPrices, sbuxDailyPrices)\nmsftPrices = to.monthly(msftDailyPrices, OHLC=FALSE)\nsp500Prices = to.monthly(sp500DailyPrices, OHLC=FALSE)\nsbuxPrices = to.monthly(sbuxDailyPrices, OHLC=FALSE)\n# set sample to match book chapter\nsmpl = \"1998-01::2012-05\"\nmsftPrices = msftPrices[smpl]\nsp500Prices = sp500Prices[smpl]\nsbuxPrices = sbuxPrices[smpl]\n# calculate returns\nmsftRetS = Return.calculate(msftPrices, method=\"simple\")\nsp500RetS = Return.calculate(sp500Prices, method=\"simple\")\nsbuxRetS = Return.calculate(sbuxPrices, method=\"simple\")\nmsftRetS = msftRetS[-1]\nsp500RetS = sp500RetS[-1]\nsbuxRetS = sbuxRetS[-1]\nmsftRetC = log(1 + msftRetS)\nsp500RetC = log(1 + sp500RetS)\nsbuxRetC = log(1 + sbuxRetS)\n# merged data set\ncerRetS = merge(msftRetS, sbuxRetS, sp500RetS)\ncerRetC = merge(msftRetC, sbuxRetC, sp500RetC)\ncolnames(cerRetS) = colnames(cerRetC) = c(\"MSFT\", \"SBUX\", \"SP500\")\nhead(cerRetC, n=3)\n\n```\n\n## Plot data\n\n```{r}\nmy.panel <- function(...) {   \n   lines(...)   \n   abline(h=0) \n} \nplot.zoo(cerRetC, lwd=2, col=\"blue\", ylim=c(-0.5, 0.3),          \n         main=\"\", panel=my.panel)\n```\n\n## Testing $H_0: \\mu = 0$ vs. $H_1: \\mu \\ne 0$\n\nBrute force calculation of t-statistic\n\n1. Get estimates of $\\mu$\n\n```{r}\nn.obs = nrow(cerRetC)\nmuhat.vals = apply(cerRetC, 2, mean)\nmuhat.vals\n```\n\n2. Calculate SEs\n\n```{r}\n# calculate standard errors\nsigmahat.vals = apply(cerRetC, 2, sd)\nse.muhat = sigmahat.vals/sqrt(n.obs)\nse.muhat\n```\n\n3. Calculate t-statistics\n\n```{r}\nt.stats = muhat.vals/se.muhat\nabs(t.stats)\n```\n\n4. Apply decision rule:  all t-statistics have absolute value less than 2, so do not reject null hypothesis at 5% level of significance.\n\n## P-value calculation\n\nP-value = significance level at which null hypothesis is just rejected\n\n```{r}\n# calculate 2-sided p-value\n2*(1-pnorm(abs(t.stats)))\n```\n\nAll p-values are less than 5%, so do not reject null hypothesis at 5% level.\n\n## Testing $H_0: \\rho_{ij} = 0.5$ vs. $H_1: \\rho_{ij} \\ne 0.5$\n\nBrute force calculation of t-statistic\n\n1. Get estimates of $\\rho_{ij$\n\n```{r}\ncorhat.mat = cor(cerRetC)\nrhohat.vals = corhat.mat[lower.tri(corhat.mat)]\nnames(rhohat.vals) = c(\"MSFT.SBUX\", \"MSFT.SP500\", \"SBUX.SP500\")\nrhohat.vals\n```\n\n2. Calculate SEs\n\n```{r}\n# calculate standard errors\nse.rhohat = (1 - rhohat.vals^2)/sqrt(n.obs)\nse.rhohat\n```\n\n3. Calculate t-statistics\n\n```{r}\nt.stats = (rhohat.vals - 0.5)/se.rhohat\nabs(t.stats)\n```\n\n4. Calculate p-values\n\n```{r}\n2*(1-pnorm(abs(t.stats)))\n```\n\n\n## Use 95% confidence interval to perform hypothesis test\n\n$H_0: \\mu = 0$ vs. $H_1: \\mu \\ne 0$ is rejected at the 5% level if $\\mu = 0$ is not in the 95% confidence interval.\n\n```{r}\n# calculate 95% confidence interval\nlower = muhat.vals - 2*se.muhat\nupper = muhat.vals + 2*se.muhat\ncbind(lower, upper)\n```\n\nHere, $\\mu = 0$ is in all of the 95% confidence intervals so we do not reject $H_0: \\mu = 0$ vs. $H_1: \\mu \\ne 0$ at the 5% level for any asset.\n\n## Use 95% confidence interval to perform hypothesis test\n\n$H_0: \\rho_{ij} = 0.5$ vs. $H_1: \\rho_{ij} \\ne 0.5$ is rejected at the 5% level if $\\rho_{ij} = 0.5$ is not in the 95% confidence interval.\n\n```{r}\nlower = rhohat.vals - 2*se.rhohat\nupper = rhohat.vals + 2*se.rhohat\ncbind(lower, upper)\n```\n\n\n## Test for normal distribution\n\nGraphical descriptive statistics suggest that the distribution of MSFT returns have fatter tails than the normal distribution.\n\n```{r}\nfourPanelPlot(cerRetC[, \"MSFT\"])\n```\n\nHowever, graphical diagnostics are not a formal statistical test.\n\n## Jarque-Bera test for normality\n\nExcess kurtosis value indicates non-normality.\n\n```{r}\nmsft.skew = skewness(cerRetC[, \"MSFT\"])\nmsft.ekurt = kurtosis(cerRetC[, \"MSFT\"])\nmsft.skew\nmsft.ekurt\n```\n\nJB statistic confirms non-mormality\n\n```{r}\nJB = n.obs*(msft.skew^2 + 0.25*msft.ekurt^2)/6\nJB\n```\n\nJB > 6 so reject $H_0: r_t \\sim N(\\mu, \\sigma^2)$ at 5% level.\n\n## R package **tseries** function <code>jarque.bera.test()</code>\n\n```{r}\nlibrary(tseries)\njarque.bera.test(cerRetC[, \"MSFT\"])\n```\n\nHere, \"X-squared\" denotes the JB statistic. The small p-value indicates rejection of null hypothesis at any reasonable significance level.\n\n## Test for no serial correlation\n\nThe R function <code>acf()</code> computes the sample autocorrelations $\\hat{\\rho_j}$, plots them and shows the critical values for rejecting the null hypothesis $H_0: \\rho_j = 0$\n\n```{r}\nacf(cerRetC[, \"MSFT\"], lwd=2)\n```\n\nHere, $\\hat{rho_1}$ and $\\hat{rho_3}$ look to be statistically differnet from zero at the 5% level.\n\n## Compute rolling means\n\nUse the **zoo** function <code>rollapply()</code> to compute rolling mean estimates.\n\n```{r}\n# compute rolling means over 24 month windows\nroll.muhat = rollapply(cerRetC[, \"MSFT\"], width=24,\n                       FUN=mean, align=\"right\")\nclass(roll.muhat)\n# first 24 values are NA\nroll.muhat[23:25]\n```\n\n## Plot rolling means with returns\n\n```{r}\nplot.zoo(merge(roll.muhat,cerRetC[, \"MSFT\"]), plot.type=\"single\",\n     main=\"24 month rolling means for MSFT\",ylab=\"returns\",\n     lwd=c(2,2), col=c(\"blue\",\"orange\"))\nabline(h=0)\ngrid()\nlegend(x=\"bottomleft\",legend=c(\"Rolling mean\",\"Monthly returns\"),\n       lwd=c(2,2), col=c(\"blue\",\"orange\"))\n```\n\n* Rolling means start out positive during dot-com boom\n* Drop negative during dot-com bust\n* close to zero during run-up to financial crisis\n* Dip negative during financial crisis\n* become positive after financial crisis\n\n## 24-month rolling means for S&P500 index \n\n```{r}\nroll.muhat = rollapply(cerRetC[, \"SP500\"], width=24,\n                       FUN=mean, align=\"right\")\nplot.zoo(merge(roll.muhat,cerRetC[, \"SP500\"]), plot.type=\"single\",\n     main=\"24-month rolling means for SP500\",ylab=\"returns\",\n     lwd=c(2,2), col=c(\"blue\",\"orange\"))\nabline(h=0)\ngrid()\nlegend(x=\"bottomleft\",legend=c(\"Rolling mean\",\"Monthly returns\"),\n       lwd=c(2,2), col=c(\"blue\",\"orange\"))\n```\n\n## 24-month rolling means for MSFT and S&P 500 index \n\n```{r}\nroll.muhat = rollapply(cerRetC, width=24, by.column=TRUE,                       \n                       FUN=mean, align=\"right\") \nclass(roll.muhat)\nhead(na.omit(roll.muhat), n=3)\n```\n\n\n## Compute rolling volatility ($\\sigma$) estimates\n\nUse the **zoo** function <code>rollapply()</code> to compute rolling $\\sigma$ estimates.\n\n```{r}\n# compute rolling sd over 24 month windows\nroll.sigmahat = rollapply(cerRetC[, \"MSFT\"], width=24,\n                       FUN=sd, align=\"right\")\nclass(roll.sigmahat)\n# first 24 values are NA\nroll.sigmahat[23:25]\n```\n\n## Plot rolling volatility with returns\n\n```{r}\nplot.zoo(merge(roll.sigmahat,cerRetC[, \"MSFT\"]), plot.type=\"single\",\n     main=\"24-month rolling volatility for MSFT\",ylab=\"returns\",\n     lwd=c(2,2), col=c(\"blue\",\"orange\"))\nabline(h=0)\ngrid()\nlegend(x=\"bottomleft\",legend=c(\"Rolling sd\",\"Monthly returns\"),\n       lwd=c(2,2), col=c(\"blue\",\"orange\"))\n```\n\n* Rolling means start out large during dot-com boom/bust\n* Decrease during run-up to financial crisis\n* increase slightly during financial crisis\n\n\n## 24-month rolling volatility for S&P500 index \n\n```{r}\nroll.sigmahat = rollapply(cerRetC[, \"SP500\"], width=24,\n                       FUN=sd, align=\"right\")\nplot.zoo(merge(roll.sigmahat,cerRetC[, \"SP500\"]), plot.type=\"single\",\n     main=\"24-month rolling volatility for SP500\",ylab=\"returns\",\n     lwd=c(2,2), col=c(\"blue\",\"orange\"))\nabline(h=0)\ngrid()\nlegend(x=\"bottomleft\",legend=c(\"Rolling sd\",\"Monthly returns\"),\n       lwd=c(2,2), col=c(\"blue\",\"orange\"))\n```\n\n## Compute rolling correlations\n\nTo compute rolling correlations, you need to write a function to return the pairwise correlation etween two returns.\n\n```{r}\nrhohat = function(x) {\n\tcor(x)[1,2]\n}\n```\n\nThen use <code>rollapply()</code> with this function.\n\n```{r}\nroll.rhohat = rollapply(cerRetC[,c(\"MSFT\",\"SP500\")],\n                       width=24,FUN=rhohat, by.column=FALSE,\n                       align=\"right\")\nclass(roll.rhohat)\nroll.rhohat[23:25]\n```\n\n## Plot rolling correlations ($\\rho_{ij}$)\n\n```{r}\nplot.zoo(roll.rhohat, main=\"24-month rolling correlations b/w MSFT and SP500\",\n     ylim=c(0,1), lwd=2, col=\"blue\", ylab=\"rho.hat\")\nabline(h=0)   \ngrid()\n```\n\n* Correlations start out around 0.6\n* Drop close to zero in mid 2005\n* Increase during run up to financial crisis\n* Reach 0.8 after financial crisis\n\n## Summary of Hypothesis Testing in CER Model\n\n* Hypothesis tests about $\\mu$ are not very powerful because $SE(\\hat{\\mu})$ is typically very large\n* Can ofter reject the null hypothesis that monthly returns are normally distributed\n* Typically cannot reject the null hypothesis that monthly returns are serially uncorrelated\n* Rolling window estimates indicate that $\\mu$, $\\sigma$ and $\\rho_{ij}$ are non constant over time (covariance stationary assumption is suspect over long time spans)\n",
    "created" : 1476563795591.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1852345417",
    "id" : "66C8232A",
    "lastKnownWriteTime" : 1476572353,
    "last_content_update" : 1476572353687,
    "path" : "C:/Users/ezivot/Dropbox/FinBook/R Presentations/hypothesisTestingCerModel.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}